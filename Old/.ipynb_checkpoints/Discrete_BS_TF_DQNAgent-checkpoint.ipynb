{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9965a059",
   "metadata": {},
   "source": [
    "Source: https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial#environment  \n",
    "  \n",
    "Needs the conda enviroment 'tfenv' activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88e538d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments.gym_wrapper import GymWrapper\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs.tensor_spec import to_array_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Environment\n",
    "from envs.discrete_BS_for_tf_agents import BSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1091aa3",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5062c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = int(1e+6) # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 5  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 2048  # @param {type:\"integer\"}\n",
    "learning_rate = 5e-4  # @param {type:\"number\"}\n",
    "log_interval = 2000  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = int(5e+4)  # @param {type:\"integer\"}\n",
    "eval_interval = 50000  # @param {type:\"integer\"}\n",
    "\n",
    "target_update_tau=0.05 # @param {type:\"float\"}\n",
    "gamma=1.0 # @param {type:\"float\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cb005",
   "metadata": {},
   "source": [
    "**Discrete Action Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e41ec247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec: BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-1.0, maximum=1.0)\n",
      "Discretized Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=9)\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=9)\n",
      "[[-1.         -0.7777778  -0.5555556  -0.33333334 -0.11111111  0.11111111\n",
      "   0.33333334  0.5555556   0.7777778   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "\n",
    "# Parameters:\n",
    "# - mu:    risky asset return \n",
    "# - sigma: risky asset variance\n",
    "# - r:     riskless rate\n",
    "# - T:     investment horizon\n",
    "# - dt:    time-step size\n",
    "# - V_0:   initial PF wealth\n",
    "# - U_2:   utility function for terminal wealth (default: math.log)\n",
    "\n",
    "# Market parameters\n",
    "mu = 0.06\n",
    "sigma = 0.2\n",
    "r = 0.04\n",
    "T = 2\n",
    "dt = 0.5\n",
    "V_0 = 100\n",
    "\n",
    "# Number of discrete Actions\n",
    "num_actions = 10\n",
    "\n",
    "env = BSEnv(mu=mu, sigma=sigma, r=r, T=T, dt=dt, V_0=V_0)\n",
    "\n",
    "\n",
    "# Wrapper for creating a PyEnvironment\n",
    "env = GymWrapper(gym_env=env)\n",
    "print('Action Spec:', env.action_spec())\n",
    "\n",
    "\n",
    "# Wrapper for discretising the action space, necessary for the DQN Algorithm\n",
    "env = wrappers.ActionDiscretizeWrapper(env, num_actions=num_actions)\n",
    "print('Discretized Action Spec:', env.action_spec())\n",
    "print(env._discrete_spec)\n",
    "discrete_actions = np.array(env._action_map, dtype='float32')\n",
    "print(discrete_actions)\n",
    "\n",
    "\n",
    "# Wrapper Py Env -> TFPyEnv\n",
    "env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbdfaae",
   "metadata": {},
   "source": [
    "**DQN Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ceb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (64, 64)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(num_units,\n",
    "                                 activation=tf.keras.activations.relu,\n",
    "                                 kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, \n",
    "                                                                 mode='fan_in',\n",
    "                                                                 distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ec9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(time_step_spec=env.time_step_spec(),\n",
    "                           action_spec=env.action_spec(),\n",
    "                           q_network=q_net,\n",
    "                           optimizer=optimizer,\n",
    "                           target_update_tau=target_update_tau,\n",
    "                           gamma=gamma,\n",
    "                           td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                           train_step_counter=train_step_counter)\n",
    "\n",
    "'''Args:\n",
    "time_step_spec: tf_agents.trajectories.TimeStep,\n",
    "    action_spec: tf_agents.typing.types.NestedTensorSpec,\n",
    "    q_network: tf_agents.networks.Network,\n",
    "    optimizer: tf_agents.typing.types.Optimizer,\n",
    "    observation_and_action_constraint_splitter: Optional[types.Splitter] = None,\n",
    "    epsilon_greedy: Optional[types.FloatOrReturningFloat] = 0.1,\n",
    "    n_step_update: int = 1,\n",
    "    boltzmann_temperature: Optional[types.FloatOrReturningFloat] = None,\n",
    "    emit_log_probability: bool = False,\n",
    "    target_q_network: Optional[tf_agents.networks.Network] = None,\n",
    "    target_update_tau: tf_agents.typing.types.Float = 1.0,\n",
    "    target_update_period: int = 1,\n",
    "    td_errors_loss_fn: Optional[tf_agents.typing.types.LossFn] = None,\n",
    "    gamma: tf_agents.typing.types.Float = 1.0,\n",
    "    reward_scale_factor: tf_agents.typing.types.Float = 1.0,\n",
    "    gradient_clipping: Optional[types.Float] = None,\n",
    "    debug_summaries: bool = False,\n",
    "    summarize_grads_and_vars: bool = False,\n",
    "    train_step_counter: Optional[tf.Variable] = None,\n",
    "    name: Optional[Text] = None\n",
    "'''\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161033e",
   "metadata": {},
   "source": [
    "**Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48ec798",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c71343",
   "metadata": {},
   "source": [
    "**Metrics and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f6aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=int(1e+4)):\n",
    "    print('Evaluating the policy for {} episodes.'.format(num_episodes))\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30713e2c",
   "metadata": {},
   "source": [
    "**Compute the average return on a random policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bb8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the policy for 50000 episodes.\n",
      "Mean utility from terminal wealth: 4.824016571044922\n"
     ]
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                                env.action_spec())\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(env, random_policy, num_eval_episodes)\n",
    "print('Mean utility from terminal wealth: {}'.format(avg_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2083bf",
   "metadata": {},
   "source": [
    "**Replay buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd4725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad76b5",
   "metadata": {},
   "source": [
    "**Data Collection**  \n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605ba7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ga63key\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(environment, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(environment, policy, buffer)\n",
    "\n",
    "collect_data(env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4533b9a",
   "metadata": {},
   "source": [
    "**Training the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b326602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ga63key\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 2000: loss = 1.0103696584701538\n",
      "step = 4000: loss = 0.2228882908821106\n",
      "step = 6000: loss = 0.07938289642333984\n",
      "step = 8000: loss = 0.021146968007087708\n",
      "step = 10000: loss = 0.018775414675474167\n",
      "step = 12000: loss = 0.012959991581737995\n",
      "step = 14000: loss = 0.02182796038687229\n",
      "step = 16000: loss = 0.028528697788715363\n",
      "step = 18000: loss = 0.011393306776881218\n",
      "step = 20000: loss = 0.01281267311424017\n",
      "step = 22000: loss = 0.009818936698138714\n",
      "step = 24000: loss = 0.012357273139059544\n",
      "step = 26000: loss = 0.010445471853017807\n",
      "step = 28000: loss = 0.00931775476783514\n",
      "step = 30000: loss = 0.008950250223279\n",
      "step = 32000: loss = 0.01190035231411457\n",
      "step = 34000: loss = 0.011024242267012596\n",
      "step = 36000: loss = 0.008310029283165932\n",
      "step = 38000: loss = 0.009324021637439728\n",
      "step = 40000: loss = 0.008813743479549885\n",
      "step = 42000: loss = 0.008521078154444695\n",
      "step = 44000: loss = 0.008382742293179035\n",
      "step = 46000: loss = 0.0077675506472587585\n",
      "step = 48000: loss = 0.008177820593118668\n",
      "step = 50000: loss = 0.008645427413284779\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 50000: Average Return = 4.834919452667236\n",
      "step = 52000: loss = 0.01010148972272873\n",
      "step = 54000: loss = 0.009039873257279396\n",
      "step = 56000: loss = 0.010202774778008461\n",
      "step = 58000: loss = 0.0075552621856331825\n",
      "step = 60000: loss = 0.008581387810409069\n",
      "step = 62000: loss = 0.009019812569022179\n",
      "step = 64000: loss = 0.008431771770119667\n",
      "step = 66000: loss = 0.015276162885129452\n",
      "step = 68000: loss = 0.009385783225297928\n",
      "step = 70000: loss = 0.008979855105280876\n",
      "step = 72000: loss = 0.007911543361842632\n",
      "step = 74000: loss = 0.011517594568431377\n",
      "step = 76000: loss = 0.007856464013457298\n",
      "step = 78000: loss = 0.008619418367743492\n",
      "step = 80000: loss = 0.008596478030085564\n",
      "step = 82000: loss = 0.007906932383775711\n",
      "step = 84000: loss = 0.008858625777065754\n",
      "step = 86000: loss = 0.007203465327620506\n",
      "step = 88000: loss = 0.007734215818345547\n",
      "step = 90000: loss = 0.008314955979585648\n",
      "step = 92000: loss = 0.00695010693743825\n",
      "step = 94000: loss = 0.008009498007595539\n",
      "step = 96000: loss = 0.009520536288619041\n",
      "step = 98000: loss = 0.00735556660220027\n",
      "step = 100000: loss = 0.008604662492871284\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 100000: Average Return = 4.849005222320557\n",
      "step = 102000: loss = 0.0072277607396245\n",
      "step = 104000: loss = 0.007980873808264732\n",
      "step = 106000: loss = 0.008223569951951504\n",
      "step = 108000: loss = 0.008801481686532497\n",
      "step = 110000: loss = 0.00805395096540451\n",
      "step = 112000: loss = 0.008164792321622372\n",
      "step = 114000: loss = 0.007649218663573265\n",
      "step = 116000: loss = 0.007442791946232319\n",
      "step = 118000: loss = 0.009125471115112305\n",
      "step = 120000: loss = 0.008286923170089722\n",
      "step = 122000: loss = 0.008647509850561619\n",
      "step = 124000: loss = 0.010933523066341877\n",
      "step = 126000: loss = 0.011178834363818169\n",
      "step = 128000: loss = 0.00891347136348486\n",
      "step = 130000: loss = 0.007701666094362736\n",
      "step = 132000: loss = 0.007390513084828854\n",
      "step = 134000: loss = 0.008221214637160301\n",
      "step = 136000: loss = 0.008442459627985954\n",
      "step = 138000: loss = 0.008047929033637047\n",
      "step = 140000: loss = 0.007198173552751541\n",
      "step = 142000: loss = 0.008891606703400612\n",
      "step = 144000: loss = 0.008174249902367592\n",
      "step = 146000: loss = 0.007830440998077393\n",
      "step = 148000: loss = 0.007712760008871555\n",
      "step = 150000: loss = 0.008953635580837727\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 150000: Average Return = 4.841067790985107\n",
      "step = 152000: loss = 0.007804194465279579\n",
      "step = 154000: loss = 0.00985109806060791\n",
      "step = 156000: loss = 0.008557761088013649\n",
      "step = 158000: loss = 0.011349672451615334\n",
      "step = 160000: loss = 0.008431031368672848\n",
      "step = 162000: loss = 0.009635461494326591\n",
      "step = 164000: loss = 0.00862764474004507\n",
      "step = 166000: loss = 0.008061610162258148\n",
      "step = 168000: loss = 0.009379206225275993\n",
      "step = 170000: loss = 0.011634839698672295\n",
      "step = 172000: loss = 0.009344067424535751\n",
      "step = 174000: loss = 0.008144077844917774\n",
      "step = 176000: loss = 0.0073088910430669785\n",
      "step = 178000: loss = 0.007530272006988525\n",
      "step = 180000: loss = 0.00913158617913723\n",
      "step = 182000: loss = 0.008845656178891659\n",
      "step = 184000: loss = 0.009133929386734962\n",
      "step = 186000: loss = 0.00825730711221695\n",
      "step = 188000: loss = 0.008338145911693573\n",
      "step = 190000: loss = 0.008172933012247086\n",
      "step = 192000: loss = 0.00689407903701067\n",
      "step = 194000: loss = 0.0067032333463430405\n",
      "step = 196000: loss = 0.010084050707519054\n",
      "step = 198000: loss = 0.007832939736545086\n",
      "step = 200000: loss = 0.007957711815834045\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 200000: Average Return = 4.831326484680176\n",
      "step = 202000: loss = 0.008315598592162132\n",
      "step = 204000: loss = 0.007713967934250832\n",
      "step = 206000: loss = 0.008358122780919075\n",
      "step = 208000: loss = 0.007911045104265213\n",
      "step = 210000: loss = 0.0072378250770270824\n",
      "step = 212000: loss = 0.008160149678587914\n",
      "step = 214000: loss = 0.008299279026687145\n",
      "step = 216000: loss = 0.0077566104009747505\n",
      "step = 218000: loss = 0.009318593889474869\n",
      "step = 220000: loss = 0.008633259683847427\n",
      "step = 222000: loss = 0.006931542884558439\n",
      "step = 224000: loss = 0.007159479893743992\n",
      "step = 226000: loss = 0.007880492135882378\n",
      "step = 228000: loss = 0.0070608025416731834\n",
      "step = 230000: loss = 0.007660731207579374\n",
      "step = 232000: loss = 0.006777939386665821\n",
      "step = 234000: loss = 0.0076225511729717255\n",
      "step = 236000: loss = 0.0073667122051119804\n",
      "step = 238000: loss = 0.006586221046745777\n",
      "step = 240000: loss = 0.007765104062855244\n",
      "step = 242000: loss = 0.007413445971906185\n",
      "step = 244000: loss = 0.007249630056321621\n",
      "step = 246000: loss = 0.007456168532371521\n",
      "step = 248000: loss = 0.006878180429339409\n",
      "step = 250000: loss = 0.006540762726217508\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 250000: Average Return = 4.84832763671875\n",
      "step = 252000: loss = 0.0070809535682201385\n",
      "step = 254000: loss = 0.007086285389959812\n",
      "step = 256000: loss = 0.007201740518212318\n",
      "step = 258000: loss = 0.006842465605586767\n",
      "step = 260000: loss = 0.006998913362622261\n",
      "step = 262000: loss = 0.007666471879929304\n",
      "step = 264000: loss = 0.007506643421947956\n",
      "step = 266000: loss = 0.008768239058554173\n",
      "step = 268000: loss = 0.007657819893211126\n",
      "step = 270000: loss = 0.007830291986465454\n",
      "step = 272000: loss = 0.007719352841377258\n",
      "step = 274000: loss = 0.009066060185432434\n",
      "step = 276000: loss = 0.007510660216212273\n",
      "step = 278000: loss = 0.00823257677257061\n",
      "step = 280000: loss = 0.007662130985409021\n",
      "step = 282000: loss = 0.008806060999631882\n",
      "step = 284000: loss = 0.009078333154320717\n",
      "step = 286000: loss = 0.0073194606229662895\n",
      "step = 288000: loss = 0.008890229277312756\n",
      "step = 290000: loss = 0.008937378413975239\n",
      "step = 292000: loss = 0.008214781060814857\n",
      "step = 294000: loss = 0.007393199019134045\n",
      "step = 296000: loss = 0.007810703478753567\n",
      "step = 298000: loss = 0.008446622639894485\n",
      "step = 300000: loss = 0.008315838873386383\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 300000: Average Return = 4.837699890136719\n",
      "step = 302000: loss = 0.008789713494479656\n",
      "step = 304000: loss = 0.008144838735461235\n",
      "step = 306000: loss = 0.007923388853669167\n",
      "step = 308000: loss = 0.007175290957093239\n",
      "step = 310000: loss = 0.007711777929216623\n",
      "step = 312000: loss = 0.008190562948584557\n",
      "step = 314000: loss = 0.008138393051922321\n",
      "step = 316000: loss = 0.008040279150009155\n",
      "step = 318000: loss = 0.007078224793076515\n",
      "step = 320000: loss = 0.006446436513215303\n",
      "step = 322000: loss = 0.008319947868585587\n",
      "step = 324000: loss = 0.007702430244535208\n",
      "step = 326000: loss = 0.00757240504026413\n",
      "step = 328000: loss = 0.0077415648847818375\n",
      "step = 330000: loss = 0.008396970108151436\n",
      "step = 332000: loss = 0.007600591983646154\n",
      "step = 334000: loss = 0.008337678387761116\n",
      "step = 336000: loss = 0.007989599369466305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 338000: loss = 0.008502820506691933\n",
      "step = 340000: loss = 0.00807330571115017\n",
      "step = 342000: loss = 0.007554173469543457\n",
      "step = 344000: loss = 0.008952561765909195\n",
      "step = 346000: loss = 0.008458087220788002\n",
      "step = 348000: loss = 0.009820684790611267\n",
      "step = 350000: loss = 0.009388848207890987\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 350000: Average Return = 4.821167469024658\n",
      "step = 352000: loss = 0.00864244531840086\n",
      "step = 354000: loss = 0.007514560595154762\n",
      "step = 356000: loss = 0.006852386984974146\n",
      "step = 358000: loss = 0.008471691980957985\n",
      "step = 360000: loss = 0.007283431477844715\n",
      "step = 362000: loss = 0.006943564862012863\n",
      "step = 364000: loss = 0.006982368417084217\n",
      "step = 366000: loss = 0.0071289557963609695\n",
      "step = 368000: loss = 0.0073880706913769245\n",
      "step = 370000: loss = 0.00797315500676632\n",
      "step = 372000: loss = 0.007002999074757099\n",
      "step = 374000: loss = 0.008908970281481743\n",
      "step = 376000: loss = 0.008361323736608028\n",
      "step = 378000: loss = 0.009191377088427544\n",
      "step = 380000: loss = 0.0077191563323140144\n",
      "step = 382000: loss = 0.00799102708697319\n",
      "step = 384000: loss = 0.007465403527021408\n",
      "step = 386000: loss = 0.006992036011070013\n",
      "step = 388000: loss = 0.00809446256607771\n",
      "step = 390000: loss = 0.00807803776115179\n",
      "step = 392000: loss = 0.007863009348511696\n",
      "step = 394000: loss = 0.007661422714591026\n",
      "step = 396000: loss = 0.007873092778027058\n",
      "step = 398000: loss = 0.0068368446081876755\n",
      "step = 400000: loss = 0.0074619147926568985\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 400000: Average Return = 4.839897155761719\n",
      "step = 402000: loss = 0.007395331282168627\n",
      "step = 404000: loss = 0.00866985134780407\n",
      "step = 406000: loss = 0.007599851116538048\n",
      "step = 408000: loss = 0.007257860153913498\n",
      "step = 410000: loss = 0.0076206582598388195\n",
      "step = 412000: loss = 0.007787411101162434\n",
      "step = 414000: loss = 0.007501670159399509\n",
      "step = 416000: loss = 0.007306407671421766\n",
      "step = 418000: loss = 0.008203191682696342\n",
      "step = 420000: loss = 0.008113095536828041\n",
      "step = 422000: loss = 0.008288799785077572\n",
      "step = 424000: loss = 0.009362028911709785\n",
      "step = 426000: loss = 0.007662699092179537\n",
      "step = 428000: loss = 0.008279507979750633\n",
      "step = 430000: loss = 0.008652547374367714\n",
      "step = 432000: loss = 0.007742843125015497\n",
      "step = 434000: loss = 0.008425967767834663\n",
      "step = 436000: loss = 0.009389798156917095\n",
      "step = 438000: loss = 0.008333813399076462\n",
      "step = 440000: loss = 0.009918807074427605\n",
      "step = 442000: loss = 0.008297555148601532\n",
      "step = 444000: loss = 0.00786488689482212\n",
      "step = 446000: loss = 0.007400594651699066\n",
      "step = 448000: loss = 0.007895157672464848\n",
      "step = 450000: loss = 0.008130678907036781\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 450000: Average Return = 4.84731388092041\n",
      "step = 452000: loss = 0.008303957059979439\n",
      "step = 454000: loss = 0.007054424379020929\n",
      "step = 456000: loss = 0.007330633699893951\n",
      "step = 458000: loss = 0.007325252052396536\n",
      "step = 460000: loss = 0.007782924920320511\n",
      "step = 462000: loss = 0.007750616408884525\n",
      "step = 464000: loss = 0.008393552154302597\n",
      "step = 466000: loss = 0.00817567016929388\n",
      "step = 468000: loss = 0.008000293746590614\n",
      "step = 470000: loss = 0.007124966010451317\n",
      "step = 472000: loss = 0.008242236450314522\n",
      "step = 474000: loss = 0.007319358177483082\n",
      "step = 476000: loss = 0.007274482864886522\n",
      "step = 478000: loss = 0.0080696577206254\n",
      "step = 480000: loss = 0.008425658568739891\n",
      "step = 482000: loss = 0.007165788672864437\n",
      "step = 484000: loss = 0.007599697448313236\n",
      "step = 486000: loss = 0.008029395714402199\n",
      "step = 488000: loss = 0.007302375510334969\n",
      "step = 490000: loss = 0.008789263665676117\n",
      "step = 492000: loss = 0.0077431341633200645\n",
      "step = 494000: loss = 0.007221921347081661\n",
      "step = 496000: loss = 0.007651341147720814\n",
      "step = 498000: loss = 0.007772182114422321\n",
      "step = 500000: loss = 0.007306339219212532\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 500000: Average Return = 4.788205623626709\n",
      "step = 502000: loss = 0.007488200441002846\n",
      "step = 504000: loss = 0.008428862318396568\n",
      "step = 506000: loss = 0.00939456932246685\n",
      "step = 508000: loss = 0.007115861400961876\n",
      "step = 510000: loss = 0.007906870916485786\n",
      "step = 512000: loss = 0.007326132617890835\n",
      "step = 514000: loss = 0.008229892700910568\n",
      "step = 516000: loss = 0.008179428987205029\n",
      "step = 518000: loss = 0.007380060385912657\n",
      "step = 520000: loss = 0.008302008733153343\n",
      "step = 522000: loss = 0.007536543998867273\n",
      "step = 524000: loss = 0.009363722987473011\n",
      "step = 526000: loss = 0.008975967764854431\n",
      "step = 528000: loss = 0.008317695930600166\n",
      "step = 530000: loss = 0.007543233223259449\n",
      "step = 532000: loss = 0.008398883044719696\n",
      "step = 534000: loss = 0.00790979154407978\n",
      "step = 536000: loss = 0.007429706864058971\n",
      "step = 538000: loss = 0.007632334716618061\n",
      "step = 540000: loss = 0.0077878134325146675\n",
      "step = 542000: loss = 0.00782977044582367\n",
      "step = 544000: loss = 0.007272183895111084\n",
      "step = 546000: loss = 0.007848462089896202\n",
      "step = 548000: loss = 0.008699269965291023\n",
      "step = 550000: loss = 0.007946927100419998\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 550000: Average Return = 4.8352508544921875\n",
      "step = 552000: loss = 0.007785596884787083\n",
      "step = 554000: loss = 0.00789007730782032\n",
      "step = 556000: loss = 0.009431777521967888\n",
      "step = 558000: loss = 0.0075545948930084705\n",
      "step = 560000: loss = 0.008210496976971626\n",
      "step = 562000: loss = 0.008503172546625137\n",
      "step = 564000: loss = 0.007313935086131096\n",
      "step = 566000: loss = 0.00723774079233408\n",
      "step = 568000: loss = 0.008323850110173225\n",
      "step = 570000: loss = 0.00803648866713047\n",
      "step = 572000: loss = 0.007856438867747784\n",
      "step = 574000: loss = 0.007795015349984169\n",
      "step = 576000: loss = 0.007455556653439999\n",
      "step = 578000: loss = 0.0071947029791772366\n",
      "step = 580000: loss = 0.00807702261954546\n",
      "step = 582000: loss = 0.008308076299726963\n",
      "step = 584000: loss = 0.008695291355252266\n",
      "step = 586000: loss = 0.008565748110413551\n",
      "step = 588000: loss = 0.007830830290913582\n",
      "step = 590000: loss = 0.00716898450627923\n",
      "step = 592000: loss = 0.007261601276695728\n",
      "step = 594000: loss = 0.00775541365146637\n",
      "step = 596000: loss = 0.008659176528453827\n",
      "step = 598000: loss = 0.007532512303441763\n",
      "step = 600000: loss = 0.00835498794913292\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 600000: Average Return = 4.8324809074401855\n",
      "step = 602000: loss = 0.00849415548145771\n",
      "step = 604000: loss = 0.008181107230484486\n",
      "step = 606000: loss = 0.007837198674678802\n",
      "step = 608000: loss = 0.007748030126094818\n",
      "step = 610000: loss = 0.008050162345170975\n",
      "step = 612000: loss = 0.007240840699523687\n",
      "step = 614000: loss = 0.007994705811142921\n",
      "step = 616000: loss = 0.00749990064650774\n",
      "step = 618000: loss = 0.0064301625825464725\n",
      "step = 620000: loss = 0.008159572258591652\n",
      "step = 622000: loss = 0.007912509143352509\n",
      "step = 624000: loss = 0.008494582027196884\n",
      "step = 626000: loss = 0.008184852078557014\n",
      "step = 628000: loss = 0.007971560582518578\n",
      "step = 630000: loss = 0.008819501847028732\n",
      "step = 632000: loss = 0.00721964705735445\n",
      "step = 634000: loss = 0.007447460200637579\n",
      "step = 636000: loss = 0.007824404165148735\n",
      "step = 638000: loss = 0.007944716140627861\n",
      "step = 640000: loss = 0.008281733840703964\n",
      "step = 642000: loss = 0.0066473279148340225\n",
      "step = 644000: loss = 0.007856840267777443\n",
      "step = 646000: loss = 0.007327479310333729\n",
      "step = 648000: loss = 0.007858438417315483\n",
      "step = 650000: loss = 0.007399522699415684\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 650000: Average Return = 4.81680154800415\n",
      "step = 652000: loss = 0.008077649399638176\n",
      "step = 654000: loss = 0.008497172966599464\n",
      "step = 656000: loss = 0.007024658378213644\n",
      "step = 658000: loss = 0.007945748046040535\n",
      "step = 660000: loss = 0.007560988422483206\n",
      "step = 662000: loss = 0.007627325598150492\n",
      "step = 664000: loss = 0.0072942390106618404\n",
      "step = 666000: loss = 0.0070997304283082485\n",
      "step = 668000: loss = 0.007235965691506863\n",
      "step = 670000: loss = 0.007963032461702824\n",
      "step = 672000: loss = 0.007333621382713318\n",
      "step = 674000: loss = 0.007019920740276575\n",
      "step = 676000: loss = 0.007412220351397991\n",
      "step = 678000: loss = 0.0074414219707250595\n",
      "step = 680000: loss = 0.007187326438724995\n",
      "step = 682000: loss = 0.007885990664362907\n",
      "step = 684000: loss = 0.007868457585573196\n",
      "step = 686000: loss = 0.0065217092633247375\n",
      "step = 688000: loss = 0.007066729012876749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 690000: loss = 0.007089423015713692\n",
      "step = 692000: loss = 0.00716762151569128\n",
      "step = 694000: loss = 0.008396727964282036\n",
      "step = 696000: loss = 0.007732383441179991\n",
      "step = 698000: loss = 0.008850166574120522\n",
      "step = 700000: loss = 0.007748821750283241\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 700000: Average Return = 4.837616920471191\n",
      "step = 702000: loss = 0.007974003441631794\n",
      "step = 704000: loss = 0.007337827701121569\n",
      "step = 706000: loss = 0.007769365329295397\n",
      "step = 708000: loss = 0.00788129772990942\n",
      "step = 710000: loss = 0.007444266229867935\n",
      "step = 712000: loss = 0.007488827221095562\n",
      "step = 714000: loss = 0.008045416325330734\n",
      "step = 716000: loss = 0.007917731069028378\n",
      "step = 718000: loss = 0.007337326183915138\n",
      "step = 720000: loss = 0.008049129508435726\n",
      "step = 722000: loss = 0.008586622774600983\n",
      "step = 724000: loss = 0.008146551437675953\n",
      "step = 726000: loss = 0.00793667696416378\n",
      "step = 728000: loss = 0.008519532158970833\n",
      "step = 730000: loss = 0.0071082510985434055\n",
      "step = 732000: loss = 0.006829125341027975\n",
      "step = 734000: loss = 0.00675796065479517\n",
      "step = 736000: loss = 0.006886372342705727\n",
      "step = 738000: loss = 0.00732496427372098\n",
      "step = 740000: loss = 0.007578369230031967\n",
      "step = 742000: loss = 0.006933184340596199\n",
      "step = 744000: loss = 0.007322466000914574\n",
      "step = 746000: loss = 0.0076778051443398\n",
      "step = 748000: loss = 0.007984846830368042\n",
      "step = 750000: loss = 0.008346131071448326\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 750000: Average Return = 4.847220420837402\n",
      "step = 752000: loss = 0.007219188846647739\n",
      "step = 754000: loss = 0.00850690808147192\n",
      "step = 756000: loss = 0.006735901813954115\n",
      "step = 758000: loss = 0.006235308013856411\n",
      "step = 760000: loss = 0.00716003030538559\n",
      "step = 762000: loss = 0.006565800867974758\n",
      "step = 764000: loss = 0.006813172250986099\n",
      "step = 766000: loss = 0.006853653118014336\n",
      "step = 768000: loss = 0.007315136957913637\n",
      "step = 770000: loss = 0.006782093551009893\n",
      "step = 772000: loss = 0.006926815491169691\n",
      "step = 774000: loss = 0.007137982174754143\n",
      "step = 776000: loss = 0.007723333779722452\n",
      "step = 778000: loss = 0.007310955785214901\n",
      "step = 780000: loss = 0.007271547801792622\n",
      "step = 782000: loss = 0.008260907605290413\n",
      "step = 784000: loss = 0.006919031031429768\n",
      "step = 786000: loss = 0.007930919528007507\n",
      "step = 788000: loss = 0.007337688934057951\n",
      "step = 790000: loss = 0.007447478361427784\n",
      "step = 792000: loss = 0.010197465308010578\n",
      "step = 794000: loss = 0.007536483928561211\n",
      "step = 796000: loss = 0.006751648616045713\n",
      "step = 798000: loss = 0.00718871783465147\n",
      "step = 800000: loss = 0.00701155187562108\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 800000: Average Return = 4.848372936248779\n",
      "step = 802000: loss = 0.007310896180570126\n",
      "step = 804000: loss = 0.007987312972545624\n",
      "step = 806000: loss = 0.007743580266833305\n",
      "step = 808000: loss = 0.007991691119968891\n",
      "step = 810000: loss = 0.0070754149928689\n",
      "step = 812000: loss = 0.007225770503282547\n",
      "step = 814000: loss = 0.007662826217710972\n",
      "step = 816000: loss = 0.008261607959866524\n",
      "step = 818000: loss = 0.007524631917476654\n",
      "step = 820000: loss = 0.00772148696705699\n",
      "step = 822000: loss = 0.006980343721807003\n",
      "step = 824000: loss = 0.006949094124138355\n",
      "step = 826000: loss = 0.00874166190624237\n",
      "step = 828000: loss = 0.007315265946090221\n",
      "step = 830000: loss = 0.006897677201777697\n",
      "step = 832000: loss = 0.0075944457203149796\n",
      "step = 834000: loss = 0.007096324115991592\n",
      "step = 836000: loss = 0.007762211374938488\n",
      "step = 838000: loss = 0.007037285715341568\n",
      "step = 840000: loss = 0.0072901928797364235\n",
      "step = 842000: loss = 0.007546728942543268\n",
      "step = 844000: loss = 0.00736408494412899\n",
      "step = 846000: loss = 0.007764361333101988\n",
      "step = 848000: loss = 0.00776261929422617\n",
      "step = 850000: loss = 0.007789045572280884\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 850000: Average Return = 4.851930618286133\n",
      "step = 852000: loss = 0.007700708229094744\n",
      "step = 854000: loss = 0.007150004617869854\n",
      "step = 856000: loss = 0.007250011432915926\n",
      "step = 858000: loss = 0.007615440990775824\n",
      "step = 860000: loss = 0.007174517959356308\n",
      "step = 862000: loss = 0.0072948504239320755\n",
      "step = 864000: loss = 0.007318933494389057\n",
      "step = 866000: loss = 0.007139682304114103\n",
      "step = 868000: loss = 0.007872926071286201\n",
      "step = 870000: loss = 0.007062630727887154\n",
      "step = 872000: loss = 0.007350947707891464\n",
      "step = 874000: loss = 0.008002834394574165\n",
      "step = 876000: loss = 0.007013267837464809\n",
      "step = 878000: loss = 0.0072707524523139\n",
      "step = 880000: loss = 0.006641956977546215\n",
      "step = 882000: loss = 0.00677130650728941\n",
      "step = 884000: loss = 0.007225749082863331\n",
      "step = 886000: loss = 0.007359244395047426\n",
      "step = 888000: loss = 0.007278852164745331\n",
      "step = 890000: loss = 0.007134635467082262\n",
      "step = 892000: loss = 0.006338819861412048\n",
      "step = 894000: loss = 0.007474939804524183\n",
      "step = 896000: loss = 0.0068692476488649845\n",
      "step = 898000: loss = 0.007007115054875612\n",
      "step = 900000: loss = 0.007632913067936897\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 900000: Average Return = 4.848572731018066\n",
      "step = 902000: loss = 0.006904056295752525\n",
      "step = 904000: loss = 0.008625298738479614\n",
      "step = 906000: loss = 0.006924937013536692\n",
      "step = 908000: loss = 0.007982322946190834\n",
      "step = 910000: loss = 0.007468535099178553\n",
      "step = 912000: loss = 0.007339976262301207\n",
      "step = 914000: loss = 0.00758194038644433\n",
      "step = 916000: loss = 0.007789283059537411\n",
      "step = 918000: loss = 0.007503192406147718\n",
      "step = 920000: loss = 0.006826340686529875\n",
      "step = 922000: loss = 0.00781430397182703\n",
      "step = 924000: loss = 0.007104343734681606\n",
      "step = 926000: loss = 0.007253486663103104\n",
      "step = 928000: loss = 0.007335365284234285\n",
      "step = 930000: loss = 0.009023364633321762\n",
      "step = 932000: loss = 0.007550287526100874\n",
      "step = 934000: loss = 0.006889635231345892\n",
      "step = 936000: loss = 0.00726296566426754\n",
      "step = 938000: loss = 0.007669809274375439\n",
      "step = 940000: loss = 0.006897950544953346\n",
      "step = 942000: loss = 0.007511524949222803\n",
      "step = 944000: loss = 0.006738828960806131\n",
      "step = 946000: loss = 0.007711102720350027\n",
      "step = 948000: loss = 0.007486354559659958\n",
      "step = 950000: loss = 0.008456990122795105\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 950000: Average Return = 4.847738742828369\n",
      "step = 952000: loss = 0.007297960110008717\n",
      "step = 954000: loss = 0.007501950953155756\n",
      "step = 956000: loss = 0.00671555707231164\n",
      "step = 958000: loss = 0.006297516170889139\n",
      "step = 960000: loss = 0.0067656985484063625\n",
      "step = 962000: loss = 0.0062517039477825165\n",
      "step = 964000: loss = 0.006863788235932589\n",
      "step = 966000: loss = 0.007062801159918308\n",
      "step = 968000: loss = 0.0069086989387869835\n",
      "step = 970000: loss = 0.006837017834186554\n",
      "step = 972000: loss = 0.007067521568387747\n",
      "step = 974000: loss = 0.006772111635655165\n",
      "step = 976000: loss = 0.0066494932398200035\n",
      "step = 978000: loss = 0.007342737168073654\n",
      "step = 980000: loss = 0.007292022462934256\n",
      "step = 982000: loss = 0.007501997984945774\n",
      "step = 984000: loss = 0.007534763775765896\n",
      "step = 986000: loss = 0.006821053102612495\n",
      "step = 988000: loss = 0.006761153694242239\n",
      "step = 990000: loss = 0.007944382727146149\n",
      "step = 992000: loss = 0.007476994767785072\n",
      "step = 994000: loss = 0.007239800877869129\n",
      "step = 996000: loss = 0.006680851802229881\n",
      "step = 998000: loss = 0.007896287366747856\n",
      "step = 1000000: loss = 0.006683518178761005\n",
      "Evaluating the policy for 50000 episodes.\n",
      "step = 1000000: Average Return = 4.844286918640137\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# The average return before training\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a59d27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "064e6f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7d05403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: \n",
      " [[ 0.       63.109745]]\n",
      "Action: \n",
      " PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6])>, state=(), info=())\n",
      "Observation: \n",
      " [[ 0.5    62.0285]]\n",
      "Action: \n",
      " PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6])>, state=(), info=())\n",
      "Observation: \n",
      " [[ 1.       64.420364]]\n",
      "Action: \n",
      " PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6])>, state=(), info=())\n",
      "Observation: \n",
      " [[ 1.5     66.58635]]\n",
      "Action: \n",
      " PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6])>, state=(), info=())\n",
      "Observation: \n",
      " [[ 2.      68.16845]]\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print(\"Observation: \\n\",time_step.observation.numpy())\n",
    "\n",
    "while not time_step.is_last():\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    time_step = env.step(action_step.action)\n",
    "    print(\"Action: \\n\", action_step)\n",
    "    print(\"Observation: \\n\", time_step.observation.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04489fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_value_surface(agent, discr_actions, times, wealths):\n",
    "    '''Plots the Action-value function of the critic network learned by the DDPG agent for different points in time\n",
    "       across wealth and risky asset allocation dimensions.\n",
    "    \n",
    "    Args:\n",
    "        - critic: [tf_agents.critic_network] The used action-value network\n",
    "        - times: [np.array] A numpy array with points in time to evaluate the action-value function at\n",
    "        - wealths: [np.array] A numpy array with the wealth levels to be used\n",
    "        - risky_asset_allocations: [np.array] A numpy array with the inv. in the risky asset to be used\n",
    "    '''\n",
    "    \n",
    "    def _q_values(agent, t, wealths):\n",
    "        '''\n",
    "        Returns a numpy array of shape wealths.shape with the respective q_values.\n",
    "\n",
    "        Args:\n",
    "            - critic: [tf_agents.critic_network] The used action-value network\n",
    "            - t: [float] Time to be used as first dimension for the observation [t, wealth]\n",
    "            - X: [np.ravel(np.meshgrid)] Risky asset allocations\n",
    "            - Y: [np.ravel(np.meshgrid)] Wealth levels\n",
    "        '''\n",
    "        if t == 0:\n",
    "            return np.array([[agent._q_network(tf.constant([[t, wealth]]), step_type=tf.constant([[0]]),training=False)] for wealth in wealths])\n",
    "        elif t < T:\n",
    "            return np.array([[agent._q_network(tf.constant([[t, wealth]]), step_type=tf.constant([[1]]),training=False)] for wealth in wealths])\n",
    "        else:\n",
    "            return np.array([[agent._q_network(tf.constant([[t, wealth]]), step_type=tf.constant([[2]]),training=False)] for wealth in wealths]) \n",
    "    \n",
    "    Zs = []\n",
    "    # Generate a seperate plot for each point in time\n",
    "    for t in times:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')                        \n",
    "        X, Y = np.meshgrid(discr_actions, wealths)\n",
    "        zs = np.array(_q_values(agent=agent, t=t, wealths=np.ravel(Y)))\n",
    "        Z = zs.reshape(X.shape)\n",
    "        ax.plot_wireframe(X, Y, Z, color=\"black\")\n",
    "\n",
    "        ax.set_xlabel('investment in risky asset')\n",
    "        ax.set_ylabel('wealth')\n",
    "        ax.set_zlabel('Q-values')\n",
    "        plt.title(\"Learned Q-value surface at time t={}\".format(t))\n",
    "\n",
    "        plt.show()\n",
    "        Zs.append(Z)\n",
    "    \n",
    "    return(Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38c4328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.array([0.0, 0.5, 1.0, 1.5, 2.0], dtype='float32')\n",
    "wealths = np.arange(50, 200, 5, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce680412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.9773107, 3.9860425, 3.9966226, 3.988876 , 3.9990597, 4.005432 ,\n",
      "        4.006585 , 4.0016036, 4.003415 , 4.006079 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.077136 , 4.0877438, 4.0970907, 4.088749 , 4.0998154, 4.1062317,\n",
      "        4.1072755, 4.1018677, 4.1045084, 4.106286 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.163744 , 4.1759796, 4.184256 , 4.175398 , 4.187231 , 4.1936846,\n",
      "        4.194634 , 4.188857 , 4.192217 , 4.1932254]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.2365294, 4.2501335, 4.25751  , 4.2482185, 4.2606955, 4.267181 ,\n",
      "        4.2680507, 4.261963 , 4.265928 , 4.2662897]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.309315 , 4.3242874, 4.330765 , 4.321039 , 4.33416  , 4.3406773,\n",
      "        4.341467 , 4.3350687, 4.339638 , 4.3393536]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.3821   , 4.3984404, 4.404019 , 4.393859 , 4.407624 , 4.4141726,\n",
      "        4.414883 , 4.408174 , 4.413348 , 4.4124174]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.4446807, 4.462197 , 4.467002 , 4.456469 , 4.4707875, 4.4773636,\n",
      "        4.4780054, 4.4710293, 4.4767237, 4.475237 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5046406, 4.5232835, 4.527348 , 4.5164576, 4.5313063, 4.5379086,\n",
      "        4.538485 , 4.531253 , 4.5374455, 4.535426 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.5646   , 4.5843697, 4.587693 , 4.5764456, 4.591825 , 4.5984535,\n",
      "        4.598964 , 4.5914764, 4.598167 , 4.5956154]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.6245594, 4.6454563, 4.648039 , 4.6364336, 4.6523438, 4.658998 ,\n",
      "        4.6594434, 4.6516995, 4.6588883, 4.655804 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.681365 , 4.7033296, 4.70521  , 4.693266 , 4.7096786, 4.716358 ,\n",
      "        4.7167416, 4.708755 , 4.716416 , 4.7128267]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.725348 , 4.7481394, 4.7494764, 4.7372704, 4.754072 , 4.7607703,\n",
      "        4.7611055, 4.7529316, 4.7609577, 4.756978 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.7693315, 4.7929497, 4.7937436, 4.781275 , 4.7984657, 4.8051834,\n",
      "        4.8054705, 4.7971087, 4.8055   , 4.8011303]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.8133144, 4.837759 , 4.8380094, 4.8252783, 4.8428583, 4.849595 ,\n",
      "        4.8498344, 4.8412848, 4.850042 , 4.845281 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.856997 , 4.8822627, 4.8819733, 4.8689823, 4.8869486, 4.8937044,\n",
      "        4.8938956, 4.88516  , 4.8942795, 4.889131 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.898596 , 4.9246435, 4.92384  , 4.9106007, 4.928935 , 4.935709 ,\n",
      "        4.935855 , 4.926942 , 4.936407 , 4.930889 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.939589 , 4.966407 , 4.9650974, 4.9516134, 4.9703107, 4.9771023,\n",
      "        4.9772034, 4.968115 , 4.9779215, 4.972039 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[4.9772635, 5.00479  , 5.0030146, 4.9893064, 5.0083365, 5.015145 ,\n",
      "        5.0152044, 5.0059557, 5.0160747, 5.009858 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0149384, 5.043173 , 5.0409317, 5.0269985, 5.046363 , 5.0531874,\n",
      "        5.0532055, 5.043796 , 5.054228 , 5.0476766]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.052613 , 5.081555 , 5.078849 , 5.064691 , 5.0843883, 5.0912294,\n",
      "        5.0912066, 5.081636 , 5.0923815, 5.085495 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.0881977, 5.1178093, 5.114663 , 5.100293 , 5.120305 , 5.127162 ,\n",
      "        5.1271   , 5.117378 , 5.128419 , 5.121217 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1195726, 5.149773 , 5.1462398, 5.131683 , 5.1519723, 5.1588426,\n",
      "        5.1587467, 5.1488905, 5.1601915, 5.152711 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.1509466, 5.1817374, 5.1778164, 5.1630726, 5.1836395, 5.190523 ,\n",
      "        5.1903934, 5.1804028, 5.191965 , 5.1842055]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.182322 , 5.213702 , 5.2093935, 5.1944623, 5.215307 , 5.222204 ,\n",
      "        5.22204  , 5.211916 , 5.2237387, 5.2157006]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2133265, 5.2452893, 5.2405977, 5.225482 , 5.2466006, 5.2535114,\n",
      "        5.2533135, 5.2430573, 5.2551374, 5.246824 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2393   , 5.2717514, 5.266738 , 5.2514677, 5.2728167, 5.2797384,\n",
      "        5.2795124, 5.269145 , 5.2814407, 5.272897 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.265274 , 5.298213 , 5.292879 , 5.2774534, 5.299032 , 5.305966 ,\n",
      "        5.305711 , 5.295233 , 5.307745 , 5.29897  ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.2912474, 5.3246746, 5.31902  , 5.303439 , 5.325248 , 5.3321924,\n",
      "        5.331909 , 5.3213205, 5.3340483, 5.3250427]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.31722  , 5.3511357, 5.34516  , 5.329425 , 5.351463 , 5.3584194,\n",
      "        5.3581076, 5.3474073, 5.360351 , 5.351115 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]\n",
      "\n",
      " [[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[5.343194 , 5.377598 , 5.371301 , 5.355411 , 5.377679 , 5.3846464,\n",
      "        5.3843064, 5.373496 , 5.386655 , 5.377188 ]], dtype=float32)>\n",
      "   ()]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-fa54ecbb82b6>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([[agent._q_network(tf.constant([[t, wealth]]), step_type=tf.constant([[0]]),training=False)] for wealth in wealths])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 600 into shape (30,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-5881a291fe5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearned_Q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_q_value_surface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscr_actions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscrete_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwealths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwealths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-fa54ecbb82b6>\u001b[0m in \u001b[0;36mplot_q_value_surface\u001b[1;34m(agent, discr_actions, times, wealths)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwealths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_wireframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"black\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 600 into shape (30,10)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learned_Q_values = plot_q_value_surface(agent, discr_actions=discrete_actions, times=times, wealths=wealths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af77828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values, _ = agent._q_network(tf.constant([[1.0, 100.0]]),\n",
    "                                  step_type=tf.constant([[1]]),\n",
    "                                  training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29a16ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.622141  4.642992  4.6456046 4.6340137 4.6499023 4.6565557 4.657004\n",
      "  4.64927   4.656439  4.653376 ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c0dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
