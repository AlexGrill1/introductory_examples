{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSEnv:\n",
    "    '''A simple discrete time BS environment'''\n",
    "    \n",
    "    def __init__(self, mu, sigma, r, T, delta_t, V_0=100): #S_0=100\n",
    "        #self.S_0     = S_0      # initial stock price\n",
    "        self.mu      = mu       # expected stock return\n",
    "        self.sigma   = sigma    # stock standard deviation\n",
    "        self.r       = r        # risk-free interest rate\n",
    "        self.T       = T        # investment horizon\n",
    "        self.delta_t = delta_t  # time-step size\n",
    "        self.action_space = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])   #investment in stock\n",
    "        self.n_actions = len(self.action_space)\n",
    "        \n",
    "        self.t      = 0         # time\n",
    "        self.V_t    = V_0       # initial wealth\n",
    "        self.state  = (0, V_0)  # initial state\n",
    "        self.reward = 0         # initial reward\n",
    "        \n",
    "        self.wealth_bins = None\n",
    "        \n",
    "        \n",
    "    #def sample_trajectory(self):\n",
    "    #    returns      = np.random.normal(loc=self.delta_t * self.mu, scale=math.sqrt(self.delta_t) * self.sigma, size=math.ceil(self.T / self.delta_t))\n",
    "    #    returns      = np.insert(returns, 0, 0)\n",
    "    #    stock_prices = self.S_0 * np.cumprod(1+returns)\n",
    "    #    return(stock_prices, returns)\n",
    "    \n",
    "    \n",
    "    def rewards_from_prices(self, stock_prices, utility = \"log\"):\n",
    "        rewards = np.zeros_like(stock_prices)        # all rewards until last time-step are zero\n",
    "        if utility == \"log\":\n",
    "            rewards[-1] = np.log(stock_prices[-1])   # last reward R_T = U(S_T)\n",
    "        else:\n",
    "            raise ValueError\\\n",
    "            ('utility function must be one of the following: log')\n",
    "        return(rewards)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        '''Resets the environment to the state (t=0, V_0)'''\n",
    "        \n",
    "        self.t = 0\n",
    "        self.state = (0, V_0)\n",
    "        self.reward = 0\n",
    "        return(self.state)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        '''Computes one step in BS Environment.'''\n",
    "        \n",
    "        self.t    += self.delta_t                   # Update time\n",
    "        self.V_t  *= action * (np.random.normal(loc=self.delta_t * self.mu, scale=math.sqrt(self.delta_t) * self.sigma) - self.r) + (1 + self.r)  # Update Wealth (see notes)\n",
    "        #self.state = (self.t, self.V_t)             # Update state\n",
    "        self.state = self.get_state(50, 150, 5)\n",
    "        \n",
    "        reward = np.log(self.V_t)*(self.t==self.T)  # Get reward according to log utility at terminal time step\n",
    "        done   = self.t==self.T                    # End of investment period\n",
    "        return(self.state, reward, done, self.t, self.V_t)\n",
    "    \n",
    "    \n",
    "    def get_state(self, lower, upper, delta_bin):\n",
    "        '''Computes the discrete state (t, V_t), for continuous t'''\n",
    "        \n",
    "        if self.wealth_bins is None:\n",
    "            # Create bins (0, lower] < (lower, lower + delta_bin] < (lower + delta_bin, lower + 2*delta_bin] < ... < (upper, inf]\n",
    "            self.wealth_bins = [0] + np.arange(lower, upper, delta_bin).tolist() + [float('Inf')] \n",
    "            \n",
    "        return((self.t, pd.cut(x=[self.V_t], bins=self.wealth_bins, retbins=False)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "(0.5, [(100.0, 105.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 0.5 103.53372785878672\n",
      "(1.0, [(100.0, 105.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 1.0 103.34773306267283\n",
      "(1.5, [(105.0, 110.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 1.5 105.65285294497387\n",
      "(2.0, [(105.0, 110.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 2.0 105.00599658844611\n",
      "(2.5, [(105.0, 110.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 2.5 108.60554164234779\n",
      "(3.0, [(110.0, 115.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 3.0 110.97308085190733\n",
      "(3.5, [(110.0, 115.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 3.5 112.98451219648156\n",
      "(4.0, [(115.0, 120.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 4.0 115.08814871051182\n",
      "(4.5, [(110.0, 115.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 0.0 False 4.5 112.76146875772261\n",
      "(5.0, [(110.0, 115.0]]\n",
      "Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]]) 4.734983865884769 True 5.0 113.86162164686634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.0,\n",
       " [(110.0, 115.0]]\n",
       " Categories (21, interval[float64]): [(0.0, 50.0] < (50.0, 55.0] < (55.0, 60.0] < (60.0, 65.0] ... (130.0, 135.0] < (135.0, 140.0] < (140.0, 145.0] < (145.0, inf]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BSEnv(mu=0.05, sigma=0.2, r=0.01, T=5, delta_t=0.5, V_0=100)\n",
    "\n",
    "#stock_prices, returns = model.sample_trajectory()\n",
    "#rewards = model.rewards_from_prices(stock_prices, utility = \"log\")\n",
    "\n",
    "#print(returns)\n",
    "#print(stock_prices)\n",
    "#print(rewards)\n",
    "#print(len(returns), len(stock_prices), len(rewards))\n",
    "print(model.action_space)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "new_state, reward, done, next_t, next_Wealth = model.step(0.1)\n",
    "print(new_state, reward, done, next_t, next_Wealth)\n",
    "model.get_state(50, 150, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.cut(x = df['height'],\n",
    "#                        bins = [0,25,50,100,200], \n",
    "#                        labels = [0, 1, 2,3])\n",
    "x = pd.cut(x=[15], bins=[0] + np.arange(11, 17, 2).tolist() + [float('Inf')], retbins=False, labels=False)\n",
    "x.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
