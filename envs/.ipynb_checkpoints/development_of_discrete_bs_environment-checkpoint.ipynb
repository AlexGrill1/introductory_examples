{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "781129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "def encode_wealth(wealth, wealth_bins):\n",
    "    return(pd.cut(x=[wealth], bins=wealth_bins, right=False, retbins=True)[0][0])\n",
    "    \n",
    "#def decode_wealth(discr_wealth, wealth_bins):\n",
    "#    return [wealth_bins[discr_wealth], wealth_bins[discr_wealth + 1]]\n",
    "    \n",
    "def encode_action(action, actions):\n",
    "    return(int(np.where(action == actions)[0][0]))\n",
    "\n",
    "def decode_action(action, actions):\n",
    "    return(actions[action])\n",
    "    \n",
    "\n",
    "class BSEnv(gym.Env):\n",
    "    '''Custom discrete-time Black-Scholes environment with one risky-asset and bank account'''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, mu, sigma, r, T, dt, V_0, actions, wealth_bins, U_2 = math.log):\n",
    "        '''\n",
    "        Args:\n",
    "            :params mu (float):         expected risky asset return\n",
    "            :params sigma (float):      risky asset standard deviation\n",
    "            :params r (float):          risk-less rate of return\n",
    "            :params T (float):          investment horizon\n",
    "            :params dt (float):         time-step size\n",
    "            :params V_0 (float, tuple): initial wealth, if tuple (v_d, v_u) draws initial wealth V(0) uniformly from [v_d, v_u]\n",
    "            :params actions (np.array): possible investment fractions into risky asset\n",
    "            :params wealth_bins (np.array): contains the limits of each wealth bin in ascending order\n",
    "            :params U_2 (callable):     utility function for terminal wealth\n",
    "        '''\n",
    "        \n",
    "        assert divmod(T, dt)[1] == 0        # To-Do: change to ValueError, is T 'ganzzahlig' divisible\n",
    "        super().__init__()\n",
    "        self.mu    = mu                        # risky asset return\n",
    "        self.sigma = sigma                     # risky asset volatility\n",
    "        self.r = r                             # risk-free rate (bank account return, riskless)\n",
    "        self.T = T                             # Termination time\n",
    "        self.dt = dt                           # time-step size\n",
    "        self.num_timesteps = T//dt\n",
    "        self.V_0 = V_0                         # Initial wealth\n",
    "        self.actions = actions                 # possible actions, fraction of wealth invested in risky aset\n",
    "        self.num_actions = len(self.actions)   # number of possible actions\n",
    "        self.wealth_bins = wealth_bins\n",
    "        self.U_2 = U_2\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        # Action space\n",
    "        self.action_space = spaces.Discrete(self.num_actions)\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.num_timesteps),\n",
    "            spaces.Discrete(len(self.wealth_bins))))\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        '''Execute one time step within the environment'''\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        # Decode the discrete action to a float\n",
    "        pi_t = decode_action(action, self.actions)\n",
    "        \n",
    "        # Update Wealth (see wealth dynamicy, Inv. Strategies script (by Prof. Zagst) Theorem 2.18):\n",
    "        # 1) Sample BM increment for one step\n",
    "        dW_t = np.random.normal(loc=0, scale=math.sqrt(self.dt))\n",
    "        # 2) Wealth process update via simulation of the exponent\n",
    "        self.V_t *= np.exp( (self.r + pi_t*(self.mu - self.r) - 0.5*(pi_t**2)*(self.sigma**2)) * self.dt + pi_t*self.sigma*dW_t ) \n",
    "        \n",
    "        \n",
    "        # Old\n",
    "        #self.V_t *= pi_t * (np.random.normal(loc=self.dt * self.mu, scale=math.sqrt(self.dt) * self.sigma) - self.dt*self.r) + (1 + self.dt*self.r)  # Update Wealth (see notes)\n",
    "        #self.V_t *= pi_t * self.dt * (self.mu  - self.r) + (1 + self.dt * self.r)    # stock pays deterministic higher return\n",
    "        \n",
    "        \n",
    "        self.time_state += 1      # updating time-step\n",
    "        #self.wealth_state = encode_wealth(self.V_t, self.wealth_bins)\n",
    "        \n",
    "        done = self.time_state == self.num_timesteps           # Episode is finished if termination time is reached\n",
    "        \n",
    "        reward = 0                                        # Reward is zero for each time step t<T\n",
    "        if done:                                          # Reward at termination time R_T = U(V_T)\n",
    "            reward = self.U_2(self.V_t)\n",
    "            \n",
    "        return self._get_obs(), reward, done, {}          # {} empty info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        print(encode_wealth(self.V_t, self.wealth_bins))\n",
    "        return (self.time_state, encode_wealth(self.V_t, self.wealth_bins))\n",
    "            \n",
    "    def reset(self):\n",
    "        '''Reset the state of the environment to an initial state'''\n",
    "        self.time_state   = 0                                          # setting time to zero\n",
    "        self.V_t          = self.V_0                                   # setting wealth to V_0\n",
    "        #self.wealth_state = encode_wealth(self.V_t, self.wealth_bins)  # encoding wealth V_0 \n",
    "        return self._get_obs()\n",
    "    \n",
    "    \n",
    "    #def render(self, mode='human', close=False):\n",
    "    # Render the environment to the screen\n",
    "    #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f093d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actions = np.arange(0, 1, step=0.1)   #vector of actions\n",
    "actions = np.array([0, 0.1, 1])\n",
    "lower = 90\n",
    "upper = 110\n",
    "delta_bin = 5\n",
    "wealth_bins = [0] + np.arange(lower, upper+1, delta_bin).tolist() + [float('Inf')]  # +1 as upper limit is not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a231e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 105.0)\n"
     ]
    }
   ],
   "source": [
    "model = BSEnv(mu=0.06, sigma=0.2, r=0.02, T=2, dt=0.5, V_0=100, actions=actions, wealth_bins=wealth_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b1ecf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Tuple(Discrete(4), Discrete(7))\n",
      "[100.0, 105.0)\n",
      "(1, Interval(100.0, 105.0, closed='left')) 103.06997853444643\n",
      "[105.0, 110.0)\n",
      "(2, Interval(105.0, 110.0, closed='left')) 0 False {} 105.40387315600938\n",
      "[105.0, 110.0)\n",
      "(3, Interval(105.0, 110.0, closed='left')) 0 False {} 107.89562333970801\n",
      "[110.0, inf)\n",
      "(4, Interval(110.0, inf, closed='left')) 4.70527158194037 True {} 110.52829836147174\n",
      "[110.0, inf)\n",
      "(5, Interval(110.0, inf, closed='left')) 0 False {} 110.77357777210784\n",
      "[110.0, inf)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, Interval(110.0, inf, closed='left'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.action_space)\n",
    "print(model.observation_space)\n",
    "print(model._get_obs(), model.V_t)\n",
    "new_state, reward, done, info = model.step(1)\n",
    "print(new_state, reward, done, next_Wealth, model.V_t)\n",
    "new_state, reward, done, info = model.step(1)\n",
    "print(new_state, reward, done, next_Wealth, model.V_t)\n",
    "new_state, reward, done, info = model.step(1)\n",
    "print(new_state, reward, done, next_Wealth, model.V_t)\n",
    "new_state, reward, done, info = model.step(1)\n",
    "print(new_state, reward, done, next_Wealth, model.V_t)\n",
    "\n",
    "model._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
