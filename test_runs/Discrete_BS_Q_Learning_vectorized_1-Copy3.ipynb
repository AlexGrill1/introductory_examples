{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\ga63key\\\\Desktop\\\\introductory_examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from envs.discrete_bs import BSEnv, decode_action, encode_action, encode_wealth, transform_Q_interval_to_Q_numeric, transform_Q_numeric_to_Q_interval, state_to_numeric\n",
    "from envs import plotting\n",
    "from utils.discrete_bs.functions import structure_preserving_update\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "import dill\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action space**  \n",
    "Actions denote the fraction of wealth invested in the **risky asset**. Actions are discretized with a step size of 10%, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{A}=[0, 0.1, 0.2, \\dots, 0.9, 1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions (Investment in risky asset): [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "# Define the vector of actions, discrete investment decisions in 10% steps\n",
    "actions = np.arange(0, 1.01, step=0.1)                  \n",
    "print(\"Actions (Investment in risky asset):\", actions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wealth discretisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalIndex([[0.0, 60.0), [60.0, 70.0), [70.0, 80.0), [80.0, 90.0), [90.0, 100.0) ... [120.0, 130.0), [130.0, 140.0), [140.0, 150.0), [150.0, 160.0), [160.0, inf)],\n",
      "              closed='left',\n",
      "              dtype='interval[float64]')\n"
     ]
    }
   ],
   "source": [
    "lower = 60      # upper limit of the lowest bucket (0, 90]\n",
    "upper = 160     # lower limit of the highest bucket (upper, +inf)\n",
    "delta_bin = 10  # bin-size inbetween \n",
    "wealth_bins = np.array([0] + np.arange(lower, upper+1, delta_bin).tolist() + [float('Inf')])  # +1 as upper limit is not included\n",
    "#wealth_bins = np.arange(lower, upper+1, delta_bin)\n",
    "\n",
    "print(pd.cut(x=[10], bins=wealth_bins, right=False, retbins=True)[0].categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete-time and discrete wealth Black-Scholes environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "'''\n",
    ":params mu (float):         expected risky asset return\n",
    ":params sigma (float):      risky asset standard deviation\n",
    ":params r (float):          risk-less rate of return\n",
    ":params T (float):          investment horizon\n",
    ":params dt (float):         time-step size\n",
    ":params V_0 (float, tuple): initial wealth, if tuple (v_d, v_u) draws initial wealth V(0) uniformly from [v_d, v_u]\n",
    ":params actions (np.array): possible investment fractions into risky asset\n",
    ":params wealth_bins (np.array): contains the limits of each wealth bin in ascending order\n",
    ":params U_2 (callable):     utility function for terminal wealth (default log-utility)\n",
    "'''\n",
    "\n",
    "mu=0.06\n",
    "sigma=0.2\n",
    "r=0.04\n",
    "T=5\n",
    "dt=1\n",
    "V_0=(50, 150)\n",
    "batch_size=1\n",
    "\n",
    "env = BSEnv(mu=mu, sigma=sigma, r=r, T=T, dt=dt, V_0=V_0, actions=actions, wealth_bins=wealth_bins, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [[0 Interval(70.0, 80.0, closed='left')]], Exact_wealth: [73.40349972]\n",
      "Action: \n",
      " 0.2\n",
      "Observation: [[1 Interval(70.0, 80.0, closed='left')]], Reward: [0.], Done: False, Exact_wealth: 78.1875202802778\n",
      "Action: \n",
      " 0.2\n",
      "Observation: [[2 Interval(80.0, 90.0, closed='left')]], Reward: [0.], Done: False, Exact_wealth: 80.7559045370119\n",
      "Action: \n",
      " 0.2\n",
      "Observation: [[3 Interval(80.0, 90.0, closed='left')]], Reward: [0.], Done: False, Exact_wealth: 81.07393248167864\n",
      "Action: \n",
      " 0.2\n",
      "Observation: [[4 Interval(80.0, 90.0, closed='left')]], Reward: [0.], Done: False, Exact_wealth: 85.36463185606141\n",
      "Action: \n",
      " 0.2\n",
      "Observation: [[5 Interval(80.0, 90.0, closed='left')]], Reward: [4.45277211], Done: True, Exact_wealth: 85.86464039868893\n"
     ]
    }
   ],
   "source": [
    "# Simulation of the wealth evolution in the BS market\n",
    "done = False\n",
    "print('Observation: {}, Exact_wealth: {}'.format(env.reset(), env.V_t))      # Reset the environment to state (0, 100)\n",
    "\n",
    "while not done:\n",
    "    print('Action: \\n', decode_action(2, actions))\n",
    "    next_obs, reward, done, info = env.step(2)              # Take the action 10 (i.e. 100% investment in risky asset) and observe the next state and reward\n",
    "    print('Observation: {}, Reward: {}, Done: {}, Exact_wealth: {}'.format(next_obs, reward, done, env.V_t)),      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon-Greedy Policy**\\\n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action-value function parametrisation in a BS market with log-utility function an no consumption.\n",
    "#def action_func(x, a, b, c):\n",
    "#    return a * (x**2) + b * x + c\n",
    "\n",
    "def func(data, a, b, c):\n",
    "    '''Parametrised action-value function Q in the log-utility case.'''\n",
    "    \n",
    "    t   = data[0]\n",
    "    inv = data[1]\n",
    "    v   = data[2]\n",
    "    T   = data[3]\n",
    "    dt  = data[4]\n",
    "    \n",
    "    # See master thesis\n",
    "    #z = a * (inv**2) + b * inv + np.log(v) + c * (2-t-1)\n",
    "    z = a * (inv**2) + b * inv + np.log(v) + c * (T-t-dt)\n",
    "    # Q-values for terminal states are zero\n",
    "    z[t==T]=0\n",
    "    \n",
    "    return  z\n",
    "\n",
    "\n",
    "def func_partial_derivative_inv(t, v, a, b, c):\n",
    "    '''Partial derivative w.r.t. a_pi of the parametrised \n",
    "       action-value function Q in the log-utility case.\n",
    "       \n",
    "    Args:\n",
    "    - t[float]: trading date\n",
    "    - v[float]: wealth level\n",
    "    - a,b,c[float]: fitted parameters\n",
    "    '''\n",
    "    \n",
    "    # optimum criterion: 2*a*inv + b == 0!\n",
    "    return(-b/(2*a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning Algorithm**  \n",
    "The general Q-Learning Algorithm Outline can be found under the link below.  \n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, actions, conc_func, conc_learning_rate, concavisation_start, conc_fit_every, initial_params, discount_factor = 1, epsilon = 1): \n",
    "    \"\"\" \n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "    Finds the optimal greedy policy while improving \n",
    "    following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "    :params env [gym.environment]: The environement used in the algorithm to sample trajectories.\n",
    "    :params num_episodes [Int]: Number of training episodes.\n",
    "    :params actions [np.array]: Numpy array containing the allocation possibilities to the risky asset.\n",
    "    :params conc_func [Callable]: Function parametrisation used for the update steps in action direction every iteration.\n",
    "    :params conc_learning_rate [Float]: The step_size used for a step towards the fitted action-values in the update state in action direction.\n",
    "    :params concavisation_start [Int]: Number of episode, when to start with the updates in action direction as well.\n",
    "    :params conc_fit_every[Int]: Fits the conc_func every conc_fit_every iteration.\n",
    "    :params initial_params[list[float]]: Initial parameters for scipy.optimize\n",
    "    :params discount_factor [Float]: The discount factor.\n",
    "    :params epsilon [Float]: Random exploration factor in Q-Learning.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Action value function \n",
    "    # A nested dictionary that maps \n",
    "    # state -> (action -> action-value). \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    A = defaultdict(lambda: np.zeros(env.action_space.n))     # Dictionary: Counts visits of state-actions pairs\n",
    "    P = defaultdict(lambda: [-1, 1, 1])                       # Dictionary: Contains parameters of conc_func for concavisation \n",
    "                                                              #             used as initial parameters for the next optimisation\n",
    "   \n",
    "    # Keeps track of useful statistics \n",
    "    stats = plotting.EpisodeStats( \n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "        episode_rewards = np.zeros(num_episodes))     \n",
    "       \n",
    "        \n",
    "    # Create an epsilon greedy policy function \n",
    "    # appropriately for environment action space \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "       \n",
    "        \n",
    "    # For every episode\n",
    "    returns=np.array([])\n",
    "    terminal_wealths = np.array([])\n",
    "    \n",
    "    \n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "            \n",
    "        # Reset the environment and pick the first action \n",
    "        state = state_to_numeric(env.reset()[0])\n",
    "        \n",
    "        \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # get probabilities of all actions from current state \n",
    "            action_probabilities = policy(state)\n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange( \n",
    "                      len(action_probabilities)), \n",
    "                       p = action_probabilities)\n",
    "            A[state][action] += 1\n",
    "   \n",
    "            # take action and get reward, transit to next state \n",
    "            # next_states: np.array of length batch_size\n",
    "            # rewards:     np.array of length batch_size\n",
    "            # done: True/False\n",
    "            next_states, rewards, done, _ = env.step(action)\n",
    "            # next_states: np.array of length batch size, each entry is a tuple (t, mean.v)\n",
    "            next_states = np.array([state_to_numeric(next_state) for next_state in next_states])\n",
    "            # mean_reward: float\n",
    "            mean_reward = np.mean(rewards)\n",
    "            \n",
    "   \n",
    "            # Update statistics \n",
    "            stats.episode_rewards[ith_episode] += mean_reward \n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "               \n",
    "            \n",
    "            # TD Update \n",
    "            # Selects the batch_size next best actions for each next state in next_states\n",
    "            best_next_actions = np.array([np.argmax(Q[(next_state[0], next_state[1])]) for next_state in next_states])\n",
    "            # Calculates the Q-values for each of the following states\n",
    "            next_q_values = np.array([Q[(next_state[0], next_state[1])][best_next_action] for next_state, best_next_action in zip(next_states, best_next_actions)])\n",
    "            # Calculate the td-target as the mean reward + \\beta * mean_next_q_values \n",
    "            # (the higher the batch_size the less noise in the target!!!)\n",
    "            td_target = mean_reward + discount_factor * np.mean(next_q_values)\n",
    "            # Calculate the delta\n",
    "            td_delta = td_target - Q[state][action]\n",
    "\n",
    "\n",
    "            # Update the action-value for the current state\n",
    "            Q[state][action] += (1/A[state][action]) * td_delta          # Dynamic Learning Rate alpha=1/#visits of state-action pair\n",
    "                                                                         # ensures convergence see Sutton & Barto eq. (2.7)\n",
    "\n",
    "                \n",
    "            \n",
    "            # Monotone updates\n",
    "            \n",
    "            if td_delta > 0:\n",
    "                states_to_be_updated = [s_tilde for s_tilde in Q.keys() if (s_tilde[0] == state[0]) & (s_tilde[1] > state[1]) & (Q[s_tilde][action] < Q[state][action])]\n",
    "                # get first state s_hat=(t, V_hat) with V_hat > V^* and Q(s_hat, action) >= Q[state, action]\n",
    "                s_hat_list = sorted([s for s in Q.keys() if (s[0] == state[0]) & (s[1] > state[1]) & (Q[s][action] >= Q[state][action])], key=lambda x: x[1])\n",
    "                \n",
    "                \n",
    "                # Update states in between via linear interpolation\n",
    "                if len(s_hat_list) != 0:\n",
    "                    s_hat = s_hat_list[0]\n",
    "                    for s_tilde in states_to_be_updated:\n",
    "                        Q[s_tilde][action] = Q[state][action] + (s_tilde[1] - state[1])*((Q[s_hat][action] - Q[state][action])/(s_hat[1] - state[1]))\n",
    "               \n",
    "                \n",
    "                # No s_hat, hence update all s_tilde equally Q(s_tilde, action) = Q(s^*, action)\n",
    "                else:\n",
    "                    states_to_be_updated = [s_tilde for s_tilde in Q.keys() if (s_tilde[0] == state[0]) & (s_tilde[1] > state[1])]\n",
    "                    for s_tilde in states_to_be_updated:\n",
    "                        Q[s_tilde][action] = Q[state][action]\n",
    "            \n",
    "            \n",
    "            elif td_delta < 0:\n",
    "                states_to_be_updated = [s_tilde for s_tilde in Q.keys() if (s_tilde[0] == state[0]) & (s_tilde[1] < state[1]) & (Q[s_tilde][action] > Q[state][action])]\n",
    "                # get first state s_hat=(t, V_hat) with V_hat < V^* and Q(s_hat, action) <= Q[state, action]\n",
    "                s_hat_list = sorted([s for s in Q.keys() if (s[0] == state[0]) & (s[1] < state[1]) & (Q[s][action] <= Q[state][action])], key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                \n",
    "                # Update states in between via linear interpolation\n",
    "                if len(s_hat_list) != 0:\n",
    "                    s_hat = s_hat_list[0]\n",
    "                    for s_tilde in states_to_be_updated:\n",
    "                        Q[s_tilde][action] = Q[s_hat][action] + (s_tilde[1] - s_hat[1])*((Q[state][action] - Q[s_hat][action])/(state[1] - s_hat[1]))\n",
    "                \n",
    "                \n",
    "                # No s_hat, hence update all s_tilde equally Q(s_tilde, action) = Q(s^*, action)\n",
    "                else:\n",
    "                    states_to_be_updated = [s_tilde for s_tilde in Q.keys() if (s_tilde[0] == state[0]) & (s_tilde[1] < state[1])]\n",
    "                    for s_tilde in states_to_be_updated:\n",
    "                        Q[s_tilde][action] = Q[state][action]\n",
    "                        \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            # Semi-structure preserving update\n",
    "            if ith_episode >= concavisation_start:\n",
    "                if (ith_episode - concavisation_start) % conc_fit_every == 0:\n",
    "                    Q, initial_params = structure_preserving_update(Q, actions, step_size=conc_learning_rate, func=conc_func, initial_params=initial_params, T=env.T, dt=env.dt)\n",
    "                        \n",
    "            \n",
    "            # done is True if episode terminated    \n",
    "            if done: \n",
    "                returns = np.append(returns, mean_reward)\n",
    "                terminal_wealths=np.append(terminal_wealths, env.V_t)\n",
    "                break\n",
    "                   \n",
    "            # Continue with the first next_state (simplifies, no need to average over wealths)        \n",
    "            state = (next_states[0][0], next_states[0][1])\n",
    "        \n",
    "        if ith_episode % 10000 == 0:\n",
    "            print(\"Episode: {}, Mean Return: {}, Mean Wealth (V_T): {}, Epsilon: {}\".format(ith_episode, round(returns.mean(), 3), round(terminal_wealths.mean(), 3), epsilon))\n",
    "            #print(\"td_delta:\", td_delta)\n",
    "            returns = np.array([])\n",
    "            terminal_wealths=np.array([])\n",
    "            \n",
    "        # Epsilon-Decay    \n",
    "        #if (ith_episode % 10000 == 0) & (ith_episode != 0):\n",
    "        #    epsilon *= 0.98\n",
    "        #    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "        #    alpha = 0.1\n",
    "        \n",
    "        # Alpha-Decay\n",
    "        #if (ith_episode % 30000 == 0) & (ith_episode != 0):\n",
    "        #    if alpha > 0.0011:\n",
    "        #        alpha *= 1/10\n",
    "            \n",
    "       \n",
    "    return Q, stats, A, initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Mean Return: 4.418, Mean Wealth (V_T): 82.896, Epsilon: 0.3\n",
      "Episode: 10000, Mean Return: 4.765, Mean Wealth (V_T): 127.017, Epsilon: 0.3\n",
      "Episode: 20000, Mean Return: 4.771, Mean Wealth (V_T): 127.499, Epsilon: 0.3\n",
      "Episode: 30000, Mean Return: 4.78, Mean Wealth (V_T): 128.402, Epsilon: 0.3\n",
      "Episode: 40000, Mean Return: 4.776, Mean Wealth (V_T): 127.89, Epsilon: 0.3\n",
      "Episode: 50000, Mean Return: 4.776, Mean Wealth (V_T): 128.226, Epsilon: 0.3\n",
      "Episode: 60000, Mean Return: 4.775, Mean Wealth (V_T): 127.876, Epsilon: 0.3\n",
      "Episode: 70000, Mean Return: 4.775, Mean Wealth (V_T): 127.596, Epsilon: 0.3\n",
      "Episode: 80000, Mean Return: 4.77, Mean Wealth (V_T): 127.216, Epsilon: 0.3\n",
      "Episode: 90000, Mean Return: 4.773, Mean Wealth (V_T): 127.61, Epsilon: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Training of the Agent\n",
    "num_episodes = 100000   # Training for 10 mio. Episodes\n",
    "\n",
    "# Starts timer\n",
    "t_0 = time.time()\n",
    "\n",
    "# Starts training\n",
    "Q, stats, A, fittedParameters = qLearning(env=env, \n",
    "                                          num_episodes=num_episodes,\n",
    "                                          actions=actions,\n",
    "                                          conc_func=func, \n",
    "                                          conc_learning_rate=5e-2, \n",
    "                                          concavisation_start=1e+4,\n",
    "                                          conc_fit_every=1e+3,\n",
    "                                          initial_params=[-1,0,0],\n",
    "                                          discount_factor = 1,\n",
    "                                          epsilon = 0.3)\n",
    "\n",
    "# Ends timer\n",
    "t_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process time: 932.9385025501251 s\n"
     ]
    }
   ],
   "source": [
    "# Print running time\n",
    "print(\"Process time: {} s\".format(t_1 - t_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5287512602868581"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_partial_derivative_inv(1, 100, *fittedParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: (0.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.18049905 4.29357274 4.29463296 4.29220366 4.29106454 4.30822439\n",
      " 4.31279247 4.34116819 4.31659365 4.34263133 4.34261655]\n",
      "Best Action (Investment in risky asset): 0.9\n",
      "Key: (1.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.50424165 4.51401824 4.52230542 4.52905894 4.533793   4.53462077\n",
      " 4.53444213 4.53108859 4.52653914 4.52151172 4.50787342]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (2.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.56799893 4.57712545 4.58673294 4.59232486 4.59684702 4.59882436\n",
      " 4.5975844  4.59564234 4.58998562 4.58147442 4.57336292]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (3.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.50485157 4.51615496 4.52381885 4.52987677 4.53426116 4.53463766\n",
      " 4.53664197 4.53206459 4.52872567 4.52002196 4.50915041]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (4.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.44477059 4.45442646 4.46399212 4.46885982 4.47233834 4.47548236\n",
      " 4.47359262 4.47168718 4.4690661  4.45956966 4.45337398]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (5.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (0.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.99352482 5.00395771 5.01233784 5.01820133 5.02236743 5.02421631\n",
      " 5.02377624 5.0225597  5.01724741 5.01148822 5.00001753]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.93091128 4.94155734 4.950485   4.95652464 4.95996547 4.96230324\n",
      " 4.96187299 4.96003224 4.95427177 4.94728733 4.93511059]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (2.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.95244673 4.96347048 4.97140488 4.9788582  4.98233729 4.9834781\n",
      " 4.98335343 4.98066364 4.97477133 4.96964623 4.96165636]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (3.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.89169967 4.90150247 4.90949335 4.91609563 4.92088385 4.92220342\n",
      " 4.92286292 4.9220564  4.91339939 4.90718726 4.89468722]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (4.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.83041277 4.84003221 4.84857096 4.85508524 4.85906483 4.86158225\n",
      " 4.86051281 4.8568459  4.85305943 4.8452193  4.83246705]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (5.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (0.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.07670809 5.08741096 5.09610185 5.10174048 5.10628623 5.10756135\n",
      " 5.10776948 5.10283636 5.09748165 5.09285677 5.08134014]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (1.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.09196612 5.10156958 5.11120485 5.11702314 5.12040344 5.12209537\n",
      " 5.1207843  5.1203987  5.11348316 5.10490423 5.09441754]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (2.0, Interval(150.0, 160.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.16945253 5.17932434 5.18703016 5.19346692 5.19712217 5.20093167\n",
      " 5.19903995 5.19507887 5.18808921 5.18157307 5.16790666]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (3.0, Interval(150.0, 160.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.10867777 5.11811518 5.12639174 5.13288709 5.13650106 5.13871008\n",
      " 5.13799904 5.13388998 5.1275056  5.12194293 5.11287065]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.1594697  4.18336105 4.20093443 4.20477642 4.20071783 4.21459787\n",
      " 4.20216473 4.22163719 4.23246892 4.2293323  4.23383935]\n",
      "Best Action (Investment in risky asset): 1.0\n",
      "Key: (2.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.08655491 4.086188   4.08110122 4.09334117 4.088986   4.10173572\n",
      " 4.08593298 4.10331451 4.1190317  4.12574441 4.12686657]\n",
      "Best Action (Investment in risky asset): 1.0\n",
      "Key: (3.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[3.99092371 3.99769709 3.98105279 4.0079748  3.98750961 4.00641964\n",
      " 4.00745107 4.01821246 4.02162808 4.02033889 3.99757892]\n",
      "Best Action (Investment in risky asset): 0.8\n",
      "Key: (4.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.31848501 4.32907213 4.33872313 4.34418297 4.34739977 4.34937319\n",
      " 4.34874425 4.34167624 4.34492237 4.33184414 4.32366256]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (5.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (1.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.01480439 5.02590065 5.03366556 5.0399924  5.04294314 5.04752659\n",
      " 5.04542222 5.04370455 5.03785401 5.0277227  5.02321285]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (2.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.03023829 5.04057667 5.04893263 5.05588183 5.05919118 5.06131029\n",
      " 5.05862432 5.05704776 5.05368821 5.04442277 5.03464114]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (3.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.96747305 4.97888265 4.98709004 4.99311779 4.99949365 4.99844181\n",
      " 4.99851404 4.99592722 4.98949371 4.98470969 4.97414633]\n",
      "Best Action (Investment in risky asset): 0.4\n",
      "Key: (4.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.90651314 4.91680599 4.92597516 4.9329577  4.93612881 4.9378164\n",
      " 4.93744202 4.93633719 4.93047127 4.92528408 4.9114223 ]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (5.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (2.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.86866831 4.8800576  4.88905953 4.89450267 4.89932051 4.9000475\n",
      " 4.89866928 4.89904435 4.89035875 4.88932251 4.87439734]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (5.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (0.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.80218891 4.81296164 4.82098811 4.82739003 4.83185363 4.83398249\n",
      " 4.83155176 4.83071584 4.82352618 4.81820583 4.80948986]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.73964339 4.75064593 4.75931025 4.76533487 4.77032098 4.77319593\n",
      " 4.77097296 4.76749079 4.76474689 4.75380145 4.74982053]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (2.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.77788756 4.78902864 4.79667288 4.80357851 4.80639053 4.808672\n",
      " 4.80900049 4.80572811 4.79994339 4.79399414 4.78520357]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (3.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.71648428 4.72632007 4.73578467 4.74133972 4.74319986 4.74697488\n",
      " 4.74493875 4.7455319  4.74318195 4.73252486 4.72325163]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (4.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.65549628 4.66626429 4.67532211 4.68022597 4.68570221 4.68638497\n",
      " 4.68613597 4.68347165 4.67863543 4.670202   4.66666804]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (0.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.56576775 4.57618745 4.58495453 4.59157548 4.59582999 4.59655769\n",
      " 4.59608748 4.59431476 4.5889948  4.5803751  4.57088336]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.62946682 4.64004948 4.64892501 4.65466241 4.65833104 4.65951957\n",
      " 4.65965587 4.65707614 4.65267183 4.64501856 4.63653353]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (2.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.6786323  4.68850937 4.69708544 4.70413298 4.70855921 4.70965643\n",
      " 4.70888131 4.70283191 4.70008229 4.6930696  4.68399909]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (3.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.61536563 4.62709009 4.63572232 4.6419815  4.64506783 4.64673791\n",
      " 4.6471671  4.64342878 4.6400305  4.63124468 4.62357535]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (5.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (4.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.55474042 4.56643158 4.57481299 4.58241836 4.58350294 4.58783061\n",
      " 4.58627934 4.58320067 4.58053521 4.57127206 4.56240955]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (0.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.22468569 5.23583806 5.24406777 5.24960671 5.25406864 5.25455292\n",
      " 5.25400208 5.2516818  5.24473507 5.23787866 5.22826584]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.16288455 5.17279241 5.18140328 5.18668309 5.19182937 5.19350728\n",
      " 5.19269202 5.18617012 5.18596215 5.17727435 5.16845879]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (1.0, Interval(150.0, 160.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.22944709 5.23985733 5.24714783 5.25437788 5.25399503 5.24987377\n",
      " 5.246757   5.24314822 5.23684484 5.24401598 5.22809813]\n",
      "Best Action (Investment in risky asset): 0.30000000000000004\n",
      "Key: (1.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.84145579 4.8506488  4.85951528 4.86462429 4.86961862 4.87243613\n",
      " 4.87249386 4.86953958 4.86180161 4.85813425 4.84499003]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (4.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.74623898 4.75768387 4.76601582 4.77224917 4.77664418 4.77741941\n",
      " 4.77718186 4.77274632 4.7693589  4.76176016 4.75120861]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (0.0, Interval(80.0, 90.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.69185697 4.70139791 4.70993738 4.71702817 4.72086967 4.72106991\n",
      " 4.72198872 4.71955552 4.71332231 4.70656384 4.69832993]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (5.0, Interval(90.0, 100.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (2.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.44122968 4.45239376 4.46067706 4.46624652 4.47115592 4.47319636\n",
      " 4.47202957 4.46788964 4.46454747 4.45677265 4.44657826]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (0.0, Interval(100.0, 110.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.90252549 4.91296231 4.92153557 4.92677753 4.93045192 4.9324869\n",
      " 4.93292253 4.93213207 4.92370749 4.91660801 4.90969571]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (3.0, Interval(110.0, 120.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.80772744 4.81805236 4.82677209 4.83417155 4.83750792 4.83840786\n",
      " 4.838969   4.83236141 4.82981432 4.82374303 4.8127448 ]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (5.0, Interval(120.0, 130.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (5.0, Interval(150.0, 160.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (4.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.97816834 4.98889788 4.99666891 5.0039154  5.00798786 5.00936641\n",
      " 5.00916452 5.00595688 5.00041475 4.99327571 4.98435576]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (0.0, Interval(130.0, 140.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.15331377 5.16434204 5.17265859 5.18022546 5.1832096  5.18466764\n",
      " 5.18556955 5.18042364 5.17683752 5.16858453 5.15780461]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (2.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.10124245 5.11184026 5.1202754  5.12595583 5.12983479 5.13279989\n",
      " 5.13101284 5.12697293 5.12326917 5.11684037 5.1052297 ]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (4.0, Interval(150.0, 160.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.04555315 5.05532452 5.06459764 5.06917797 5.07499307 5.07534291\n",
      " 5.07543132 5.07281799 5.06667252 5.06221846 5.05273626]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (4.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[3.89480829 3.9195011  3.9193612  3.93146575 3.91683886 3.91817021\n",
      " 3.92043565 3.91562594 3.92121974 3.901247   3.90269632]\n",
      "Best Action (Investment in risky asset): 0.30000000000000004\n",
      "Key: (5.0, Interval(60.0, 70.0, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (3.0, Interval(140.0, 150.0, closed='left'))\n",
      "State-Action Values:\n",
      "[5.03917151 5.05033139 5.05922732 5.06563369 5.06970639 5.06926357\n",
      " 5.07009397 5.06665541 5.06101694 5.05663172 5.04687045]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (3.0, Interval(70.0, 80.0, closed='left'))\n",
      "State-Action Values:\n",
      "[4.37961804 4.38993292 4.3996848  4.40743467 4.40901983 4.41009562\n",
      " 4.40894873 4.40813018 4.40251302 4.39713212 4.38613856]\n",
      "Best Action (Investment in risky asset): 0.5\n",
      "Key: (4.0, Interval(160.0, inf, closed='left'))\n",
      "State-Action Values:\n",
      "[5.23580253 5.23366834 5.23117535 5.23370818 5.23454816 5.23552998\n",
      " 5.23489193 5.23689063 5.23556626 5.2280047  5.23107018]\n",
      "Best Action (Investment in risky asset): 0.7000000000000001\n",
      "Key: (3.0, Interval(160.0, inf, closed='left'))\n",
      "State-Action Values:\n",
      "[5.2398414  5.23984187 5.23971405 5.22942979 5.23933292 5.2144099\n",
      " 5.20841664 5.19138271 5.19827909 5.18701726 5.17550841]\n",
      "Best Action (Investment in risky asset): 0.1\n",
      "Key: (2.0, Interval(160.0, inf, closed='left'))\n",
      "State-Action Values:\n",
      "[5.24762335 5.24765579 5.24757318 5.24357904 5.2395958  5.22931113\n",
      " 5.22934794 5.22011332 5.21598705 5.20613871 5.19626423]\n",
      "Best Action (Investment in risky asset): 0.1\n",
      "Key: (5.0, Interval(160.0, inf, closed='left'))\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (1.0, Interval(160.0, inf, closed='left'))\n",
      "State-Action Values:\n",
      "[5.25771143 5.2577047  5.25770433 5.25732371 5.25756592 5.25460557\n",
      " 5.24935039 5.24514315 5.24616621 5.24628219 5.23097198]\n",
      "Best Action (Investment in risky asset): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Transforms the Q-table to interval representation\n",
    "Q_interval = transform_Q_numeric_to_Q_interval(Q, wealth_bins)\n",
    "\n",
    "\n",
    "# Prints the learned Action-values for each state + the best action for each state\n",
    "for key in Q_interval.keys():\n",
    "    print(\"Key:\", key)\n",
    "    print(\"State-Action Values:\", Q_interval[key], sep=\"\\n\")\n",
    "    print(\"Best Action (Investment in risky asset):\", decode_action(np.argmax(Q_interval[key]), actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(Q, actions):\n",
    "    '''Creates a 3d Wireframe plot of the Q-value function for each state-action pair \n",
    "       and adds the predicted action (i.e. argmax_a Q(s,a)\n",
    "    \n",
    "    Args:\n",
    "    :params: Q [dict] A dictionary containing the action-values for each state\n",
    "    :params: actions [np.array] A np.array containing the possible actions\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def fun(x, y, Q, actions, t):\n",
    "        '''Help function used in plot_q_values'''\n",
    "        # Returns the Q-values for each state-action pair at time step .\n",
    "        return np.array([Q[(t,wealth)][encode_action(action, actions)] for action, wealth in zip(x,y)])\n",
    "    \n",
    "    times = sorted(list(set([t for t,_ in Q.keys()])))\n",
    "    for time in times:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        x = actions\n",
    "        y = np.array(sorted([wealth for t, wealth in Q.keys() if t == time]))\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        zs = np.array(fun(np.ravel(X), np.ravel(Y), Q, actions, time))\n",
    "        Z = zs.reshape(X.shape)\n",
    "\n",
    "        # Predicted Actions for each state\n",
    "        states = [key for key in Q.keys() if key[0] == time]\n",
    "        predicted_actions = [decode_action(np.argmax(Q[state]), actions) for state in states]\n",
    "        wealths = [wealth for _, wealth in states]\n",
    "        predicted_Q_values = [Q[state][np.argmax(Q[state])] for state in states]\n",
    "\n",
    "        ax.plot_wireframe(X, Y, Z, color=\"black\")\n",
    "\n",
    "        ax.set_xlabel('investment in risky asset')\n",
    "        ax.set_ylabel('wealth')\n",
    "        ax.set_zlabel('Q-values')\n",
    "        ax.scatter(predicted_actions, wealths, predicted_Q_values, zdir=\"z\", c=\"red\", alpha=1, label=\"Predicted Actions\")\n",
    "        plt.title(\"Learned Q-value surface (at t={})\".format(time))\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604d1480d6d549879b83cfdae85916db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897b84a016244d9085a9f30d86fcd3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47ea9f7a0ab41b3bf88cdc94509f672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864ab72043e34f778a2f4d3ec8d22b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcd90711ac8412e855872cbf3ec345c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813d9e02d6c84abba6bfd4c7885cd5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(Q, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learned_vs_optimal_policy(env, Q, actions):\n",
    "    '''Plots the learned policy derived from the action-value function Q vs. the optimal policy for logarithmic utility.\n",
    "    \n",
    "    Args:\n",
    "    :params env[gym.environment]: The environment used for collecting samples.\n",
    "    :params Q[dict]: The Q-Table.\n",
    "    :params actions[np.array]: np.array containing the possible investment choices.\n",
    "    '''\n",
    "    \n",
    "    # Construct a seperate plot for each time \n",
    "    for time in range(1, int(env.num_timesteps)):\n",
    "        # get the observed wealth levels for each time\n",
    "        wealth_levels = sorted([wealth for t, wealth in Q.keys() if t == time])\n",
    "        # Derives the investment choice from the action-value function Q for the given state\n",
    "        predicted_actions = [decode_action(np.argmax(Q[(time, wealth)]), actions) for wealth in wealth_levels]\n",
    "\n",
    "        \n",
    "        # Plots the learned policy\n",
    "        plt.plot(wealth_levels, predicted_actions, label=\"learned\")\n",
    "        # Plots the optimal policy for logarithmic utility\n",
    "        plt.plot(wealth_levels, ((env.mu - env.r)/(env.sigma**2))*np.ones(len(wealth_levels)), \"-.\", label=\"optimal\")\n",
    "        plt.title(\"Learned policy vs. optimal policy (at time t={})\".format(time*env.dt))\n",
    "        plt.xlabel(\"wealth\")\n",
    "        plt.ylabel(\"risky asset allocation\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca4e5b01654cab9df615312062fe26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learned_vs_optimal_policy(env, Q, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('sessions/Discrete_BS_Q_Learning_vectorized_1.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
